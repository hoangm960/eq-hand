{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01f8cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import math\n",
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47d0f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = HandDetector(detectionCon=0.8, maxHands=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9138a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "GESTURE = {\n",
    "    \"bass\": [0, 0, 0, 0, 0],\n",
    "    \"mid\": [0, 1, 0, 0, 0],\n",
    "    \"treble\": [0, 0, 0, 0, 1],\n",
    "    \"all\": [1, 1, 1, 1, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d0de21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_RANGE = [0, 100]\n",
    "VOLUME_RANGE = (-50, 50)\n",
    "INITIAL_VOLUME_ANGLE = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b1c5f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = 'playlist\\Gusto.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97ed265e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.exists(audio_file):\n",
    "    print(\"File exists!\")\n",
    "else:\n",
    "    print(\"File not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd66d5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\ffmpeg\\ffmpeg-2025-05-01-git-707c04fe06-full_build\\bin\\ffmpeg.exe\n"
     ]
    }
   ],
   "source": [
    "from pydub.utils import which\n",
    "print(which(\"ffmpeg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7227ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EqController import EQController\n",
    "from AudioPlayer import AudioPlayer\n",
    "\n",
    "eq_controller = EQController()\n",
    "player = AudioPlayer(audio_file, eq_controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c458631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "threading.Thread(target=player.play).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e30df44",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m hands, img = \u001b[43mdetector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfindHands\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hands) == \u001b[32m2\u001b[39m:\n\u001b[32m     13\u001b[39m     hand1, hand2 = hands\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\gitRepo\\eq-hand\\venv\\Lib\\site-packages\\cvzone\\HandTrackingModule.py:55\u001b[39m, in \u001b[36mHandDetector.findHands\u001b[39m\u001b[34m(self, img, draw, flipType)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03mFinds hands in a BGR image.\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03m:param img: Image to find the hands in.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m:param draw: Flag to draw the output on the image.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m:return: Image with or without drawings\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     54\u001b[39m imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28mself\u001b[39m.results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhands\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgRGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m allHands = []\n\u001b[32m     57\u001b[39m h, w, c = img.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\gitRepo\\eq-hand\\venv\\Lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[39m, in \u001b[36mHands.process\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> NamedTuple:\n\u001b[32m    133\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m \u001b[33;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\gitRepo\\eq-hand\\venv\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[39m, in \u001b[36mSolutionBase.process\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._graph.add_packet_to_input_stream(\n\u001b[32m    336\u001b[39m         stream=stream_name,\n\u001b[32m    337\u001b[39m         packet=\u001b[38;5;28mself\u001b[39m._make_packet(input_stream_type,\n\u001b[32m    338\u001b[39m                                  data).at(\u001b[38;5;28mself\u001b[39m._simulated_timestamp))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 1280)\n",
    "cap.set(4, 720)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    hands, img = detector.findHands(img)\n",
    "\n",
    "    if len(hands) == 2:\n",
    "        hand1, hand2 = hands\n",
    "        if hand1[\"center\"][0] < hand2[\"center\"][0]:\n",
    "            left_hand = hand2\n",
    "            right_hand = hand1\n",
    "        else:\n",
    "            left_hand = hand1\n",
    "            right_hand = hand2\n",
    "\n",
    "        fingers_left = detector.fingersUp(left_hand)\n",
    "        freq_band = \"none\" # default\n",
    "        gain = 1.0 # default\n",
    "        volume = 1.0 # default\n",
    "        \n",
    "        for control, gesture in GESTURE.items():\n",
    "            if fingers_left == gesture:\n",
    "                freq_band = control\n",
    "\n",
    "                if control == \"all\": # adjust the volume\n",
    "                    wrist = np.array(right_hand['lmList'][0])\n",
    "                    middle_tip = np.array(right_hand['lmList'][12])\n",
    "\n",
    "                    dx = middle_tip[0] - wrist[0]\n",
    "                    dy = middle_tip[1] - wrist[1]\n",
    "                    angle = -math.degrees(math.atan2(dy, dx)) - INITIAL_VOLUME_ANGLE\n",
    "                    angle_clamped = max(\n",
    "                        VOLUME_RANGE[0], min(VOLUME_RANGE[1], angle))\n",
    "                    # gain = int(angle_clamped)\n",
    "                    volume = (angle_clamped - VOLUME_RANGE[0]) / (VOLUME_RANGE[1] - VOLUME_RANGE[0])\n",
    "                    # set band name \n",
    "                    # eq_controller.set_band(control)\n",
    "                    # eq_controller.set_gain(gain)\n",
    "                    player.set_volume(volume)\n",
    "                    # assign band name and gain\n",
    "                    # freq_band = eq_controller.get_selected_band()\n",
    "                    # gain = eq_controller.get_gains()\n",
    "                    player.get_volume()\n",
    "                    \n",
    "\n",
    "                else: # adjust the eq\n",
    "                    distance, info, img = detector.findDistance(\n",
    "                        right_hand[\"lmList\"][8][:-1],\n",
    "                        right_hand[\"lmList\"][4][:-1],\n",
    "                        img\n",
    "                    )\n",
    "                    gain = max(EQ_RANGE[0], min(EQ_RANGE[1], int(distance * 0.5)))\n",
    "                    # set band name \n",
    "                    eq_controller.set_band(control)\n",
    "                    eq_controller.set_gain(gain)\n",
    "                    # assign band name and gain\n",
    "                    freq_band = eq_controller.get_selected_band()\n",
    "                    gain = eq_controller.get_gains()\n",
    "                break    \n",
    "\n",
    "        cv2.putText(img, f\"Freq Band: {freq_band}\", (10, 70),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 255), 3,)\n",
    "        cv2.putText(img, f\"Gain: {gain}\", (10, 110),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 255), 3,)\n",
    "        cv2.putText(img, f\"Volume: {int(volume*100)} %\", (10, 150),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 255), 3,)\n",
    "\n",
    "    cv2.imshow(\"Hand Gesture Equalizer Control\", img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        player.stop()\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
